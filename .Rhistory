data = train[,-10],
num.trees= 500, ## Numero de bootstrap samples y arboles a estimar. Default 500
mtry= 4,   # N. var aleatoriamente seleccionadas en cada partición
min.node.size  = 1, ## Numero minimo de observaciones en un nodo
importance="impurity")
rf
# Calculamos las predicciones
Default_hat <-predict(rf, data = test, predict.all = TRUE)$predictions
pred.rf <- as.data.frame(Default_hat)
# Calcular las probabilidades de Default
ntrees <- ncol( pred.rf )
phat.rf <- rowSums(pred.rf == 2) / ntrees
aucval_rf <- Metrics::auc(
actual = default_num[-inTrain],
predicted = phat.rf
)
aucval_rf
rf2 <- randomForest::randomForest(
Default~duration+amount+installment+age+
history+purpose+foreign+rent,
data = train,
importance = TRUE,
ntree = 500,
mtry = 4,
nodesize = 1
)
rf2
# NOTA: Este bloque de código está presentando errores cuando se compila
# el documento en RMarkdown pero no en ejecutado de forma interactiva.
# Por esta razón no se muestra el resultado de la ejecución de este bloque.
# Revisar que los factores sean iguales en train y test
all_factors <- sapply(train, is.factor)
factor_vars <- names(train)[all_factors]
for (var in factor_vars) {
train_levels <- levels(train[[var]])
test_levels <- levels(test[[var]])
if (!identical(train_levels, test_levels)) {
stop(
sprintf("Mismatch in factor levels for variable '%s':\n  Train levels: %s\n  Test levels: %s",
var,
paste(train_levels, collapse = ", "),
paste(test_levels, collapse = ", "))
)
}
}
Default_hat <- stats::predict(
rf2, # usamos objeto randomForest
newdata = test,
type = 'response',
predict.all = TRUE # para obtener la predicción de cada arbol.
)$individual # Guardamos la predicción de cada árbol en forma de dataframe
# Calcular las probabilidades de Default
ntrees <- ncol( Default_hat )
phat.rf2 <- rowSums(Default_hat == "Si") / ntrees
# Calcular y guardar AUC de random forest
aucval_rf2 <- Metrics::auc(
actual = default_num[-inTrain],
predicted = phat.rf2)
aucval_rf2 # el AUC fue 0.7069048
names(predict(
rf, # usamos objeto ranger
data = test,
predict.all = TRUE # para obtener la predicción de cada arbol.
))
names(predict(
rf2, # usamos objeto randomForest
newdata = test,
predict.all = TRUE # para obtener la predicción de cada arbol.
))
# output: [1] "aggregate"  "individual"
randomForest::varImpPlot(rf2)
imp<-rf$variable.importance
imp2<- data.frame(variables= names(imp),
importance= imp)
ggplot(imp2, aes(x = reorder(variables, importance) , y =importance )) +
geom_bar(stat = "identity", fill = "red") +
labs(title = "Importancia con `ranger` ", x = "Importance", y="Variable") +
theme_minimal() +
coord_flip()
fiveStats <- function(...) {
c(
caret::twoClassSummary(...), # Returns ROC, Sensitivity, and Specificity
caret::defaultSummary(...)  # Returns RMSE and R-squared (for regression) or Accuracy and Kappa (for classification)
)
}
ctrl<- trainControl(method = "cv",
number = 5,
summaryFunction = fiveStats,
classProbs = TRUE,
verbose=FALSE,
savePredictions = T)
mtry_grid<-expand.grid(mtry =c(2,4,6,8), # 8 incluye bagging
min.node.size= c(1, 5, 10, 20, 35, 50), #controla la complejidad del arbol
splitrule= 'gini') # tomamos gini como splitrule
mtry_grid
cv_RForest <- train(Default~duration+amount+installment+age+
history+purpose+foreign+rent,
data = train,
method = "ranger", # llamamos el paquete del metodo a utilizar
trControl = ctrl,
metric="ROC", # metrica a optimizar
tuneGrid = mtry_grid,
ntree=500)
cv_RForest
cv_RForest$finalModel
rf_pred <- predict(cv_RForest,
newdata = test,
type="prob" ## class for class prediction
)
aucval_rf <- Metrics::auc(actual = default_num,predicted =rf_pred[,2])
aucval_rf
fiveStats <- function(...) {
c(
caret::twoClassSummary(...), # Returns ROC, Sensitivity, and Specificity
caret::defaultSummary(...)  # Returns RMSE and R-squared (for regression) or Accuracy and Kappa (for classification)
)
}
ctrl<- trainControl(method = "cv",
number = 5,
summaryFunction = fiveStats,
classProbs = TRUE,
verbose=FALSE,
savePredictions = T)
adagrid<-  expand.grid(
mfinal = c( 50, 300 ,500),
maxdepth = c(1,2,5),
coeflearn = c('Breiman','Freund'))
set.seed(91519) # important set seed.
adaboost_tree <- train(Default~duration+amount+installment+age+
history+purpose+foreign+rent,
data = train,
method = "AdaBoost.M1",  # para implementar el algoritmo antes descrito
trControl = ctrl,
metric = "ROC",
tuneGrid=adagrid
)
adaboost_tree
default<- ifelse(test$Default=="Si",1,0)
pred_prob <- predict(adaboost_tree,
newdata = test,
type = "prob")
aucval_AdaBoost <- Metrics::auc(actual = default,predicted = pred_prob[,2])
aucval_AdaBoost
p_load(gbm)
grid_gbm<-expand.grid(n.trees= c( 50, 100,150),
interaction.depth=c(1,2),
shrinkage=c(0.01),
n.minobsinnode=c(5, 10))
set.seed(91519) # important set seed.
gbm_tree <- train(Default~duration+amount+installment+age+
history+purpose+foreign+rent,
data = train,
method = "gbm",
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "ROC",
verbose = FALSE
)
gbm_tree
pred_prob <- predict(gbm_tree,
newdata = test,
type = "prob")
aucval_GRboost <- Metrics::auc(actual = default,predicted = pred_prob[,2])
aucval_GRboost
p_load(xgboost)
grid_xbgoost <- expand.grid(nrounds = c(250,500),
max_depth = c(1, 2),
eta = c(0.1,  0.01),
gamma = c(0, 1),
min_child_weight = c(10, 25),
colsample_bytree = c(0.4, 0.7),
subsample = c(0.7))
grid_xbgoost
set.seed(91519) # Importante definir la semilla antes entrenar
Xgboost_tree <- train(Default~duration+amount+installment+age+
history+purpose+foreign+rent,
data = train,
method = "xgbTree",
trControl = ctrl,
tuneGrid=grid_xbgoost,
metric = "ROC",
verbosity = 0
)
Xgboost_tree
pred_prob <- predict(Xgboost_tree,
newdata = test,
type = "prob")
aucval_XGboost <- Metrics::auc(actual = default,predicted = pred_prob[,2])
aucval_XGboost
tree_plot <- xgboost::xgb.plot.tree(
model = Xgboost_tree$finalModel,
trees = 1:2,
plot_width = 1000,
plot_height = 500)
tree_plot
AUC<- data.frame(
Model= c("Tree", "CV Tree",
"Bagging", "Random Forest",
"AdaBoost", "GBoost","XGBoost"),
testAUC= c(
0.6391270, 0.7046561,
aucval_bag, aucval_rf,
aucval_AdaBoost , aucval_GRboost, aucval_XGboost)
)
AUC
# Create bar plot
ggplot(AUC, aes(x = reorder(Model, testAUC), y = testAUC)) +
geom_bar(stat = "identity", fill = "steelblue", width = 0.7) +
geom_text(aes(label = round(testAUC, 3)),
vjust = -0.5,
size = 3.5) +
labs(title = "Comparación de Rendimiento de Modelos",
subtitle = "AUC de prueba",
x = NULL,
y = "AUC (Area Under the Curve)") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank()
) +
ylim(0, max(AUC$testAUC) * 1.1)
### Codigo de procesamiento de datos
# Objetivo: Construir una base a nivel hogar con la mayor cantidad
# de variables predictivas
library(pacman)
pacman::p_load(
tidyverse, dyplr
)
#-----------ESTABLECER DIRECTORIO--------------------------
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
load("Taller2.RData")
#-1. EXPLORING DATASET ------------------------------------
train_hogares <- train_hogares |>
select(colnames(test_hogares))
train_personas <- train_personas |>
select(colnames(test_personas))
dataset <- train_hogares |>
left_join(train_personas, by = c("id", "Clase", "Dominio", "Fex_c", "Fex_dpto", "Depto"))
dataset |>
select(starts_with("P")) |>
mutate_all(as.factor) |>
summary()
# ----> 1. EXPLORING DATA SET
# MAIN STATISTICS
summary <- dataset |>
select(where(is.numeric)) |>
pivot_longer(everything(), names_to = "Variable", values_to = "Value") |>
group_by(Variable) |>
summarise(
Min = round(min(Value, na.rm = TRUE),2),
Mean = round(mean(Value, na.rm = TRUE),2),
Max = round(max(Value, na.rm = TRUE),2),
NA_count = sum(is.na(Value)),
NA_percent = round(100 * mean(is.na(Value)), 2)
)
write_csv(summary, "1.Exploracion.csv")
# ----> CREATE GROUP AGES
dataset <- dataset |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-55", "55+"),
ordered = TRUE))
library(pacman)
pacman::p_load(
tidyverse, dyplr
)
#-----------ESTABLECER DIRECTORIO--------------------------
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
load("Taller2.RData")
#-1. EXPLORING DATASET ------------------------------------
train_hogares <- train_hogares |>
select(colnames(test_hogares))
train_personas <- train_personas |>
select(colnames(test_personas))
dataset <- train_hogares |>
left_join(train_personas, by = c("id", "Clase", "Dominio", "Fex_c", "Fex_dpto", "Depto"))
dataset |>
select(starts_with("P")) |>
mutate_all(as.factor) |>
summary()
# ----> 1. EXPLORING DATA SET
# MAIN STATISTICS
summary <- dataset |>
select(where(is.numeric)) |>
pivot_longer(everything(), names_to = "Variable", values_to = "Value") |>
group_by(Variable) |>
summarise(
Min = round(min(Value, na.rm = TRUE),2),
Mean = round(mean(Value, na.rm = TRUE),2),
Max = round(max(Value, na.rm = TRUE),2),
NA_count = sum(is.na(Value)),
NA_percent = round(100 * mean(is.na(Value)), 2)
)
write_csv(summary, "1.Exploracion.csv")
# ----> CREATE GROUP AGES
dataset <- dataset |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-55", "55+"),
ordered = TRUE))
dataset <- dataset |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-55", "55+"),
ordered = TRUE))
dataset <- dataset |>
mutate(age_group = case_when(
age < 12 ~ "0-12",
age >= 12 & age < 18 ~ "12-18",
age >= 18 & age < 24 ~ "18-24",
age >= 24 & age < 55 ~ "24-55",
age >= 55 ~ "55+"
)) |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-55", "55+"),
ordered = TRUE))
summary <- dataset |>
select(where(is.numeric)) |>
pivot_longer(everything(), names_to = "Variable", values_to = "Value") |>
group_by(Variable) |>
summarise(
Min = round(min(Value, na.rm = TRUE),2),
Mean = round(mean(Value, na.rm = TRUE),2),
Max = round(max(Value, na.rm = TRUE),2),
NA_count = sum(is.na(Value)),
NA_percent = round(100 * mean(is.na(Value)), 2)
)
write_csv(summary, "1.Exploracion.csv")
SEGURIDAD_SOCIAL <- dataset|>
group_by(P6090, P6100,age_group) |>
summarise(count = n(), .groups = "drop_last") |>
mutate(total = sum(count)) |>
mutate(porcentaje = count / total)
dataset <- dataset |>
mutate(age_group = case_when(
age < 12 ~ "0-12",
age >= 12 & P < 18 ~ "12-18",
age >= 18 & age < 24 ~ "18-24",
age >= 24 & age < 55 ~ "24-55",
age >= 55 ~ "55+"
)) |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-55", "55+"),
ordered = TRUE))
dataset <- dataset |>
mutate(age_group = case_when(
P6040 < 12 ~ "0-12",
P6040 >= 12 & P6040 < 18 ~ "12-18",
P6040 >= 18 & P6040 < 24 ~ "18-24",
P6040 >= 24 & P6040 < 55 ~ "24-55",
P6040 >= 55 ~ "55+"
)) |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-55", "55+"),
ordered = TRUE))
SEGURIDAD_SOCIAL <- dataset|>
group_by(P6090, P6100,age_group) |>
summarise(count = n(), .groups = "drop_last") |>
mutate(total = sum(count)) |>
mutate(porcentaje = count / total)
JOB_TYPE <- dataset |>
group_by(age_group,P6430)|>
summarise(count = n(), .groups = "drop_last") |>
mutate(total = sum(count)) |>
mutate(porcentaje = count / total)
JOB_TYPE
View(JOB_TYPE)
JOB_TYPE <- dataset |>
group_by(age_group,P6430)|>
summarise(count = n(), .groups = "drop_last") |>
mutate(total = sum(count)) |>
mutate(porcentaje = round((count / total),2))
JOB_TYPE
View(JOB_TYPE)
write_csv(JOB_TYPE, "JOB_TYPE.csv")
JOB_TYPE <- dataset |>
group_by(age_group,P6430)|>
summarise(count = sum(is.na()), .groups = "drop_last") |>
mutate(total = sum(n())) |>
mutate(porcentaje = round((count / total),2))
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = sum(is.na(P6430)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
write_csv(JOB_TYPE, "JOB_TYPE.csv")
dataset <- dataset |>
mutate(age_group = case_when(
P6040 < 12 ~ "0-12",
P6040 >= 12 & P6040 < 18 ~ "12-18",
P6040 >= 18 & P6040 < 24 ~ "18-24",
P6040 >= 24 & P6040 < 65 ~ "24-55",
P6040 >= 65 ~ "55+"
)) |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-65", "65+"),
ordered = TRUE))
# ----> P6090 y P6100:
SEGURIDAD_SOCIAL <- dataset|>
group_by(P6090, P6100,age_group) |>
summarise(count = n(), .groups = "drop_last") |>
mutate(total = sum(count)) |>
mutate(porcentaje = count / total)
# ---> CREATING NEW VARIABLES.
# ---> PERTENECE AL RÉGIMEN SUBSIDIADO?
dataset <- dataset |>
mutate(regimen_subsidiado = as.numeric(case_when(P6100 == 3 ~ 1, TRUE ~ 0))) ;rm(SEGURIDAD_SOCIAL)
# ---> INFORMAL WORKER
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = sum(is.na(P6430)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
View(JOB_TYPE)
dataset <- dataset |>
mutate(age_group = case_when(
P6040 < 12 ~ "0-12",
P6040 >= 12 & P6040 < 18 ~ "12-18",
P6040 >= 18 & P6040 < 24 ~ "18-24",
P6040 >= 24 & P6040 < 65 ~ "24-65",
P6040 >= 65 ~ "65+"
)) |>
mutate(age_group = factor(age_group,
levels = c("0-12", "12-18", "18-24", "24-65", "65+"),
ordered = TRUE))
SEGURIDAD_SOCIAL <- dataset|>
group_by(P6090, P6100,age_group) |>
summarise(count = n(), .groups = "drop_last") |>
mutate(total = sum(count)) |>
mutate(porcentaje = count / total)
# ---> CREATING NEW VARIABLES.
# ---> PERTENECE AL RÉGIMEN SUBSIDIADO?
dataset <- dataset |>
mutate(regimen_subsidiado = as.numeric(case_when(P6100 == 3 ~ 1, TRUE ~ 0))) ;rm(SEGURIDAD_SOCIAL)
# ---> INFORMAL WORKER
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = sum(is.na(P6430)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
View(JOB_TYPE)
View(JOB_TYPE)
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = sum((P6430)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
write_csv(JOB_TYPE, "JOB_TYPE.csv"); rm(JOB_TYPE)
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = sum((P6430)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
View(JOB_TYPE)
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = sum(P6430), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
View(JOB_TYPE)
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = sum(P6430), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
JOB_TYPE <- dataset |>
group_by(age_group, P6430) |>
summarise(count = n(), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count),
porcentaje = round(count / total, 2))
View(JOB_TYPE)
write_csv(JOB_TYPE, "JOB_TYPE2.csv"); rm(JOB_TYPE)
dataset <- dataset %>%
mutate(
informal = case_when(
is.na(P6430) & is.na(regimen_subsidiado) ~ NA_real_,
P6430 %in% c(4, 5, 6, 8) & regimen_subsidiado == 1 ~ 1,
TRUE ~ 0
)
)
sum(dataset$informal)
# ---> HORAS TRABAJADAS SEMANALMENTE
HORAS_TRABAJADAS <- dataset |>
group_by(age_group, P6800) |>
summarise(count = sum(is.na(P6800)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count)) |>
mutate(porcentaje = round(count / total, 2))
View(HORAS_TRABAJADAS)
rm(HORAS_TRABAJADAS)
# ---> SEGUNDO TRABAJO
SEGUNDO_TRABAJO <- dataset |>
group_by(P7050) |>
summarise(count = sum(is.na(P6430)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count))
View(SEGUNDO_TRABAJO)
SEGUNDO_TRABAJO <- dataset |>
group_by(P7050) |>
summarise(count = sum(is.na(P7050)), .groups = "drop_last") |>
ungroup() |>
mutate(total = sum(count))
SEGUNDO_TRABAJO <- dataset |>
group_by(P7050) |>
summarise(count = sum((P7050)), .groups = "drop_last")
View(SEGUNDO_TRABAJO)
