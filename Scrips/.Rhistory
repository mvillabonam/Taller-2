"SUBEMPLEADO"
)], na.rm = TRUE)
# ----> A nivel de Hogar
test_dataset <- test_dataset %>%
group_by(id) %>%
mutate(
VULNERABILIDAD_LABORAL = sum(VULNERABILIDAD_LABORAL, na.rm = TRUE)
) %>%
ungroup()
#----> POSICIONES OCUPACIONALES DE BAJO INGRESO
# ------------------------------------------------------------------------------
# ------> EXPORTING DATA SET
# Renombramos y seleccionamos las variables finales ----
train_dataset <- train_dataset |>
ungroup()
train_dataset <- train_dataset |>
select(colnames(test_hogares),Pobre,
P6020, age_group,
P6210,P6240,regimen_subsidiado:last_col()) |>
select(-c(P5100, P5130,P5140)) |>
rename(num_cuartos = P5000,
num_dormitorios = P5010,
tipo_posesion = P5090,
sexo_jefe = P6020,
edad_jefe = age_group,
educ_jefe = P6210,
actividad_jefe = P6240)
train_dataset
train_dataset <- train_dataset |>
mutate(across(.cols = -c(id, Li, Lp, Fex_c, Fex_dpto, hacinamiento, prop_informal, median_education, int_subempleo, costo_vivienda, tasa_dependencia,RURAL, VULNERABILIDAD_LABORAL), .fns = as.factor))
train_dataset <- train_dataset |>
mutate(num_cuartos = fct_collapse(num_cuartos,
mas_de_10 = c("11", "12", "13", "14", "15", "16","18", "43", "98")),
num_dormitorios = fct_collapse(num_dormitorios,
mas_de_8 = c("9", "15")),
Nper = fct_collapse(Nper,
mas_de_16 = c("17", "18", "18", "19", "20", "21", "22", "28")),
Npersug = fct_collapse(Npersug,
mas_de_16 = c("17", "18", "18", "19", "20", "21", "22", "28")),
num_ocupados = fct_collapse(num_ocupados,
mas_de_8 = c("9", "10", "11", "14")),
num_dependientes = fct_collapse(num_dependientes,
mas_de_8 = c("9", "10", "11", "12")))
# En el test
test_dataset <- test_dataset |>
ungroup()
test_dataset <- test_dataset |>
select(colnames(test_hogares),
P6020, age_group,
P6210,P6240,regimen_subsidiado:last_col()) |>
select(-c(P5100, P5130,P5140)) |>
rename(num_cuartos = P5000,
num_dormitorios = P5010,
tipo_posesion = P5090,
sexo_jefe = P6020,
edad_jefe = age_group,
educ_jefe = P6210,
actividad_jefe = P6240)
test_dataset <- test_dataset |>
mutate(across(.cols = -c(id, Li, Lp, Fex_c, Fex_dpto, hacinamiento, prop_informal, median_education, int_subempleo, tasa_dependencia, costo_vivienda,VULNERABILIDAD_LABORAL,RURAL), .fns = as.factor))
test_dataset <- test_dataset |>
mutate(num_cuartos = fct_collapse(num_cuartos,
mas_de_10 = c("11", "12", "13", "14", "15", "16","18", "43", "98")),
num_dormitorios = fct_collapse(num_dormitorios,
mas_de_8 = c("9", "15")),
Nper = fct_collapse(Nper,
mas_de_16 = c("17", "18", "18", "19", "20", "21", "22", "28")),
Npersug = fct_collapse(Npersug,
mas_de_16 = c("17", "18", "18", "19", "20", "21", "22", "28")),
num_ocupados = fct_collapse(num_ocupados,
mas_de_8 = c("9", "10", "11", "14")),
num_dependientes = fct_collapse(num_dependientes,
mas_de_8 = c("9", "10")))
# Verificar factores
for (col in intersect(names(train_dataset), names(test_dataset))) {
if (is.factor(train_dataset[[col]])) {
test_dataset[[col]] <- factor(test_dataset[[col]], levels = levels(train_dataset[[col]]))
}
colnames(train_dataset)
colnames(train_dataset)
View(test_dataset)
colnames(test_dataset)
knitr::opts_chunk$set(echo = TRUE)
require(pacman)
p_load(
readxl,         # Read excel files
tidyverse,      # Tidy data
skimr,          # Data summary
ranger,
caret,         # Entrenamiento y evaluacion de modelos
doParallel,     # Paralelizacion
MLeval,         # Funciones para evaluar modelos con métricas gráficos.
MLmetrics,     # Colección de métricas de evaluación para modelos de machine learning.
pROC,
rpart,
rpart.plot,
gbm,
MLmetrics,     # Colección de métricas de evaluación para modelos de machine learning.
pROC,
adabag,
e1071,
ranger,         # Para bagging y random forest
randomForest,   # Para random forest
Metrics,        # Evaluation Metrics for ML
adabag,
gbm,            # Gradient Boosting
xgboost,        # XGBoosting
pROC,           # ROC curve
ParBayesianOptimization, # Selecting parameters on XGBoost
doParallel, # Paralellize data proccessing
gridExtra,
corrplot,
stargazer
)
x_variables <- c(colnames(train_dataset)[-c(1,2,3,8,9,10,11,13,14)])
skim(train_dataset)
skim(test_dataset)
train_dataset$actividad_jefe[is.na(train_dataset$actividad_jefe)] <- 6
test_dataset$num_ocupados[is.na(test_dataset$num_ocupados)] <- "mas_de_8"
test_dataset$actividad_jefe[is.na(test_dataset$actividad_jefe)] <- 6
test_dataset <- test_dataset |>
mutate(actividad_jefe = ifelse(is.na(actividad_jefe),"6", actividad_jefe))
ggplot(train_dataset, aes(x = Pobre, fill = Pobre)) +
geom_bar() +
theme_minimal() +
scale_fill_manual(values = c("Pobre" = "orange", "No_pobre"= "blue")) +
labs(x = "", y = "# de Personas")
# Pobre as a factor
train_dataset <- train_dataset %>%
mutate(Pobre = factor(Pobre, levels = c(1, 0), labels = c("Pobre", "No_pobre")))
skim(train_dataset)
skim(test_dataset)
train_dataset$actividad_jefe[is.na(train_dataset$actividad_jefe)] <- 6
test_dataset$num_ocupados[is.na(test_dataset$num_ocupados)] <- "mas_de_8"
test_dataset$actividad_jefe[is.na(test_dataset$actividad_jefe)] <- 6
test_dataset <- test_dataset |>
mutate(actividad_jefe = ifelse(is.na(actividad_jefe),"6", actividad_jefe))
ggplot(train_dataset, aes(x = Pobre, fill = Pobre)) +
geom_bar() +
theme_minimal() +
scale_fill_manual(values = c("Pobre" = "orange", "No_pobre"= "blue")) +
labs(x = "", y = "# de Personas")
multiStats <- function(...) c(twoClassSummary(...), defaultSummary(...), prSummary(...), multiClassSummary(...))
# ESTABLECIENDO EL CONTROL ---> PERMITIENDO PARALELIZACIÓN DE NÚCLEOS
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
ctrl <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = multiStats,
verboseIter = TRUE,
allowParallel = TRUE
)
#  Cross-tabulations for key relationships
poverty_vulnerability <- table(train_dataset$Pobre, train_dataset$vulnerabilidad)
print(poverty_vulnerability_table)
multiStats <- function(...) c(twoClassSummary(...), defaultSummary(...), prSummary(...), multiClassSummary(...))
# ESTABLECIENDO EL CONTROL ---> PERMITIENDO PARALELIZACIÓN DE NÚCLEOS
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
ctrl <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = multiStats,
verboseIter = TRUE,
allowParallel = TRUE
)
# OPTIMIZANDO SELECCIÓN DE PARÁMETROS
# Evauar resultados del ejercicio
fiveStats <- function(...) {
c(
caret::twoClassSummary(...), # Returns ROC, Sensitivity, Specificity
caret::defaultSummary(...)  # RMSE, R-squared/Accuracy and Kappa
)
}
#------------------- PREPARING DATA---------------------------------------------
selected_data <- train_dataset %>%
mutate(across(
.cols = -c(id, Clase, Dominio),
.fns = ~ {
# Try to convert to numeric after replacing NAs with 0
if (is.numeric(.)) {
replace_na(., 0)
} else {
suppressWarnings(as.numeric(replace_na(., "0")))
}
}
))
# -------------------------- HIPERPARAMETERS -----------------------------------
# HIPERPARÁMETROS: # árboles, profundidad, mínimo de observaciones x nodo, tasa de aprendizaje,
# nivel mínimo de reducción en la función de pérdida, porcentaje de la muestra para entrenar (subsample)
x_variables <- c(colnames(train_dataset)[-c(1,2,3,8,9,10,11,13,14)])
X <- as.matrix(selected_data[, x_variables])
y <- selected_data$Pobre  # Or whatever your target column is
y <- ifelse(selected_data$Pobre == 2, 1,0)
# 2. DMatrix
dtrain <- xgb.DMatrix(data = X, label = y)
# 3. Create custom folds
Folds <- list(
Fold1 = as.integer(seq(1, nrow(X), by = 3)),
Fold2 = as.integer(seq(2, nrow(X), by = 3)),
Fold3 = as.integer(seq(3, nrow(X), by = 3))
)
# 4. Set parameters
params <- list(
objective = "binary:logistic", # or "multi:softprob" if multiclass
eval_metric = "auc"        # can change to "auc", etc.
)
scoringFunction <- function(max_depth, min_child_weight, subsample) {
dtrain_local <- xgb.DMatrix(data = X, label = y)  # X and y from global
Pars <- list(
booster = "gbtree",
eta = 0.001,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample,
objective = "binary:logistic",
eval_metric = "auc"
)
xgbcv <- xgb.cv(
params = Pars,
data = dtrain_local,
nrounds = 100,
folds = Folds,
early_stopping_rounds = 5,
maximize = TRUE,
verbose = 0
)
return(list(
Score = max(xgbcv$evaluation_log$test_auc_mean),
nrounds = xgbcv$best_iteration
))
}
bounds <- list(
max_depth = c(1L, 20L)
, min_child_weight = c(0, 25)
, subsample = c(0.25, 1)
)
#---------------------------------- PARALELLALIZING ----------------------------
cl <- makeCluster(3)
registerDoParallel(cl)
clusterExport(cl, c("X", "y", "Folds", "scoringFunction"))
set.seed(0)
tWithPar <- system.time(
optObj <- bayesOpt(
FUN = scoringFunction,
bounds = bounds,
initPoints = 4,
iters.n = 4,
iters.k = 2,
parallel = FALSE
)
)
stopCluster(cl)
registerDoSEQ()
optObj[["scoreSummary"]]
getBestPars(optObj)
#--------------------------------------------------------------------------------
# Definiendo grilla
grid_xbgoost <- expand.grid(nrounds = c(250,500),
max_depth = c(5,15),
eta = c(0.1,0.3),
gamma = c(0, 1),
min_child_weight =c(5),
colsample_bytree = c(0.7),
subsample = c(0.5))
#--------------------------------------------------------------------------------
# Definiendo grilla
grid_xbgoost <- expand.grid(nrounds = c(500),
max_depth = c(15),
eta = c(0.01),
gamma = c(0, 1),
min_child_weight =c(5),
colsample_bytree = c(0.7),
subsample = c(0.5))
set.seed(1231)
# ---> PROBANDO EL MODELO
train_dataset_clean <- train_dataset %>%
select(all_of(c("Pobre", x_variables))) %>%
na.omit()
num_cores <- parallel::detectCores()/2
cl <- makeCluster(num_cores)
cl <- makeCluster(num_cores)
registerDoParallel(cl)
formula_xgb <- as.formula(paste("Pobre ~", paste(x_variables, collapse = " + ")))
Xgboost_tree <- train(formula_xgb,
data = train_dataset_clean,
method = "xgbTree",
trControl = ctrl,
tuneGrid=grid_xbgoost,
metric = "ROC",
verbosity = 0
)
# PREDICTIONS
test_predictions <- predict(Xgboost_tree,
newdata = test_dataset,
type = "prob")
predicted_class <- ifelse(test_predictions$Pobre > 0.5, 1, 0)
submission <- data.frame(
id = test_dataset$id,
Pobre = predicted_class
)
# ENTREGA 3
write.csv(submission, "XGBOOST.csv", row.names = FALSE, quote = FALSE)
x_variables <- c("tasa_dependencia", "VULNERABILIDAD_LABORAL", "median_education", "hacinamiento",
"costo_vivienda", "vulnerabilidad", "RURAL","recibe_subsidios", "recibe_remesas", "informal")
#- MODEL 2 ---------------------------------------------------------------------
# Definiendo grilla
grid_xbgoost <- expand.grid(nrounds = c(250,500),
max_depth = c(18),
eta = c(0.01),
gamma = c(0, 1),
min_child_weight =c(5),
colsample_bytree = c(0.7),
subsample = c(0.5))
set.seed(1231)
num_cores <- parallel::detectCores()/2
cl <- makeCluster(num_cores)
registerDoParallel(cl)
x_variables <- c("tasa_dependencia", "VULNERABILIDAD_LABORAL", "median_education", "hacinamiento",
"costo_vivienda", "vulnerabilidad", "RURAL","recibe_subsidios", "recibe_remesas", "informal")
formula_xgb <- as.formula(paste("Pobre ~", paste(x_variables, collapse = " + ")))
Xgboost_tree <- train(formula_xgb,
data = train_dataset_clean,
method = "xgbTree",
trControl = ctrl,
tuneGrid=grid_xbgoost,
metric = "ROC",
verbosity = 0
)
Xgboost_tree
# PREDICTIONS
test_predictions <- predict(Xgboost_tree,
newdata = test_dataset,
type = "prob")
predicted_class <- ifelse(test_predictions$Pobre > 0.5, 1, 0)
submission <- data.frame(
id = test_dataset$id,
Pobre = predicted_class
)
Xgboost_tree
# PREDICTIONS
test_predictions <- predict(Xgboost_tree,
newdata = test_dataset,
type = "prob")
predicted_class <- ifelse(test_predictions$Pobre > 0.5, 1, 0)
submission <- data.frame(
id = test_dataset$id,
Pobre = predicted_class
)
test_predictions
x_variables <- c("tasa_dependencia", "VULNERABILIDAD_LABORAL", "median_education", "hacinamiento",
"vulnerabilidad", "RURAL","recibe_subsidios")
formula_xgb <- as.formula(paste("Pobre ~", paste(x_variables, collapse = " + ")))
Xgboost_tree <- train(formula_xgb,
data = train_dataset_clean,
method = "xgbTree",
trControl = ctrl,
tuneGrid=grid_xbgoost,
metric = "ROC",
verbosity = 0
)
Xgboost_tree <- train(formula_xgb,
data = train_dataset_clean,
method = "xgbTree",
trControl = ctrl,
tuneGrid=grid_xbgoost,
metric = "ROC",
verbosity = 0
)
Xgboost_tree
# PREDICTIONS
test_predictions <- predict(Xgboost_tree,
newdata = test_dataset,
type = "prob")
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c("RURAL", "recibe_subsidios","vulnerabilidad",
"regimen_subsidiado","hacinamiento")
set.seed(1410)
default_lda = train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data=train_dataset,
method="lda",
trControl = ctrl)
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c("RURAL", "recibe_subsidios","vulnerabilidad",
"regimen_subsidiado","hacinamiento")
set.seed(1410)
default_lda = train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data=train_dataset,
method="lda",
trControl = ctrl)
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c("RURAL", "recibe_subsidios","vulnerabilidad",
"regimen_subsidiado","hacinamiento")
set.seed(1410)
default_lda = train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data=train_dataset,
method="lda",
trControl = ctrl)
set.seed(1410)
default_lda = train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data=train_dataset,
method="lda",
trControl = ctrl)
ctrl <- trainControl(method = "cv",
number = 5,
summaryFunction = multiStats,
classProbs = TRUE,
verbose = FALSE,
savePredictions = TRUE)
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c("RURAL", "recibe_subsidios","vulnerabilidad",
"regimen_subsidiado","hacinamiento")
set.seed(1410)
default_lda = train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data=train_dataset,
method="lda",
trControl = ctrl)
ctrl <- trainControl(method = "cv",
number = 5,
summaryFunction = multiStats,
classProbs = TRUE,
verbose = FALSE,
savePredictions = TRUE)
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c("RURAL", "recibe_subsidios","vulnerabilidad",
"regimen_subsidiado","hacinamiento")
set.seed(1410)
default_lda = train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data=train_dataset,
method="lda",
trControl = ctrl)
default_lda <- train(
formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data = train_dataset %>% drop_na(Pobre, all_of(x_variables)),
method = "lda",
trControl = ctrl
)
default_lda$results$F1
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c(colnames(train_dataset)[-c(1,2,3,8,9,10,11,13,14)])
set.seed(1410)
default_lda <- train(
formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data = train_dataset %>% drop_na(Pobre, all_of(x_variables)),
method = "lda",
trControl = ctrl
)
train_dataset$actividad_jefe[is.na(train_dataset$actividad_jefe)] <- 6
test_dataset$num_ocupados[is.na(test_dataset$num_ocupados)] <- "mas_de_8"
test_dataset$actividad_jefe[is.na(test_dataset$actividad_jefe)] <- 6
test_dataset <- test_dataset |>
mutate(actividad_jefe = ifelse(is.na(actividad_jefe),"6", actividad_jefe))
```
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c(colnames(train_dataset)[-c(1,2,3,8,9,10,11,13,14)])
set.seed(1410)
default_lda <- train(
formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data = train_dataset %>% drop_na(Pobre, all_of(x_variables)),
method = "lda",
trControl = ctrl
)
default_lda <- train(
formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data = train_dataset %>% drop_na(Pobre, all_of(x_variables)),
method = "lda",
trControl = ctrl
)
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
train_dataset$Pobre <- factor(train_dataset$Pobre, levels = c(0, 1))
x_variables <- c(colnames(train_dataset)[-c(1,2,3,8,9,10,11,13,14)])
default_lda <- train(
formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data = train_dataset %>% drop_na(Pobre, all_of(x_variables)),
method = "lda",
trControl = ctrl
)
## LDA: LINEAR DISCRIMINANT ANALYSIS (LDA)
x_variables <- c(colnames(train_dataset)[-c(1,2,3,8,9,10,11,13,14)])
set.seed(1410)
default_lda <- train(
formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
data = train_dataset %>% drop_na(Pobre, all_of(x_variables)),
method = "lda",
trControl = ctrl
)
default_lda$results$F1
write.csv(submission, "XGBOOST2.csv", row.names = FALSE, quote = FALSE)
predicted_class <- ifelse(test_predictions$Pobre > 0.3, 1, 0)
# PREDICTIONS
test_predictions <- predict(Xgboost_tree,
newdata = test_dataset,
type = "prob")
predicted_class <- ifelse(test_predictions$Pobre > 0.3, 1, 0)
submission <- data.frame(
id = test_dataset$id,
Pobre = predicted_class
)
# PREDICTIONS
test_predictions <- predict(Xgboost_tree,
newdata = test_dataset,
type = "prob")
predicted_class <- ifelse(test_predictions$Pobre > 0.3, 1, 0)
submission <- data.frame(
id = test_dataset$id,
Pobre = predicted_class
)
