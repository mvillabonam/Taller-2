---
title: "Taller 2 - G7"
author: "Grupo 7"
date: "2025-04-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(pacman)
p_load(
  readxl,         # Read excel files
  tidyverse,      # Tidy data  
  skimr,          # Data summary  
  caret,          # Entrenamiento y evaluacion de modelos
  doParallel,     # Paralelizacion
  MLeval,         # Funciones para evaluar modelos con métricas gráficos.
  #MLmetrics,     # Colección de métricas de evaluación para modelos de machine learning.
  ranger,         # Para bagging y random forest
  randomForest,   # Para random forest
  Metrics,        # Evaluation Metrics for ML
  adabag,
  gbm,            # Gradient Boosting
  xgboost,        # XGBoosting
  pROC
)
```

## Carga de los datos 
```{r Reading poverty files}

train_dataset <- read_xlsx("train_dataset.xlsx")
test_dataset <- read_xlsx("test_dataset.xlsx")

train_dataset <- train_dataset %>%
  mutate(Pobre = factor(Pobre, levels = c(1, 0), labels = c("Pobre", "No_pobre")))

```

## Exploración de datos

```{r, echo=FALSE}
skim(train_dataset)
skim(test_dataset)

train_dataset <- train_dataset |> 
  mutate(actividad_jefe = ifelse(is.na(actividad_jefe),"6", actividad_jefe))

test_dataset <- test_dataset |> 
  mutate(actividad_jefe = ifelse(is.na(actividad_jefe),"6", actividad_jefe))

```

```{r}
ggplot(train_dataset, aes(x = Pobre, fill = Pobre)) +
  geom_bar() + 
  theme_minimal() +  
  scale_fill_manual(values = c("Pobre" = "orange", "No_pobre"= "blue")) +  
  labs(x = "", y = "# de Personas")  

```

```{r Creating metric function}
multiStats <- function(...) c(twoClassSummary(...), defaultSummary(...), prSummary(...), multiClassSummary(...))

```


## 1. Logistic Regression
```{r}
# Control para caret
ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = multiStats,
                     classProbs = TRUE,
                     verbose = FALSE,
                     savePredictions = TRUE)

# Entrenar con glm binomial
set.seed(1231)

x_variables <- c(colnames(train_dataset)[-c(1,3,8,9,10,11,13,14)])

glm_caret <- train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
                   data = train_dataset,
                   method = "glm",
                   trControl = ctrl,
                   family = "binomial")
glm_caret
```


## 3. Elastic Net

## 3. CARTs

## 4. Random Forest

## 5. ADA Boost

## 6. Grad Boost

## 7. XGBoost
```{r XGBOOST}
# OPTIMIZANDO SELECCIÓN DE PARÁMETROS 
install.packages("ParBayesianOptimization")
install.packages("xgboost")
install.packages("pROC")      
# Evauar resultados del ejercicio 
fiveStats <- function(...) {
  c(
    caret::twoClassSummary(...), # Returns ROC, Sensitivity, Specificity
    caret::defaultSummary(...)  # RMSE, R-squared/Accuracy and Kappa 
  )
}

# HIPERPARÁMETROS: # árboles, profundidad, mínimo de observaciones x nodo, tasa de                     aprendizaje,  nivel mínimo de reducción en la función de                            pérdida, porcentaje de la muestra para entrenar (subsample)

# Definiendo grilla
grid_xbgoost <- expand.grid(nrounds = c(250,500),
                            max_depth = c(1,3),
                            eta = c(0.1,0.3), 
                            gamma = c(0, 1), 
                            min_child_weight =c(1000),
                            colsample_bytree = c(0.7), 
                            subsample = c(0.7))



set.seed(1231)
# ---> PROBANDO EL MODELO
x_variables <- c(
  "median_education", "costo_vivienda", "recibe_subsidios",
  "num_cuartos", "vulnerabilidad", "regimen_subsidiado",
  "hacinamiento", "num_dependientes", "Nper", "recibe_remesas",
  "tipo_posesion", "int_subempleo"
)

formula_xgb <- as.formula(paste("Pobre ~", paste(x_variables, collapse = " + ")))

Xgboost_tree <- train(formula_xgb,
  data = train_dataset, 
  method = "xgbTree", 
  trControl = ctrl,
  tuneGrid=grid_xbgoost,
  metric = "ROC",
  verbosity = 0
)         
Xgboost_tree

# PREDICTIONS 
test_predictions <- predict(Xgboost_tree,
                     newdata = test_dataset, 
                     type = "prob")
predicted_class <- ifelse(test_predictions$Pobre > 0.5, 1, 0)

submission <- data.frame(
  id = test_dataset$id,
  Pobre = predicted_class
)

# ENTREGA 3
write.csv(submission, "XGBOOST.csv", row.names = FALSE, quote = FALSE)

# GRAPHIC OF THE THREE
tree_plot <- xgboost::xgb.plot.tree(
  model = Xgboost_tree$finalModel,
  trees = 1:2,
  plot_width = 2000,
  plot_height = 1000)
tree_plot

# Verify the format
head(submission)

# GRAPHIC OF THE THREE
aucval_XGboost <- Metrics::auc(actual = default,predicted = pred_prob[,2])
aucval_XGboost 

```

## 8. Baile Bayes

## 9. LDA/QDA

## 10. Naive Bayes

## 11. KNN

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

