---
title: "Taller 2 - G7"
author: "Grupo 7"
date: "2025-04-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(pacman)
p_load(
  readxl,         # Read excel files
  tidyverse,      # Tidy data  
  skimr,          # Data summary  
  caret,          # Entrenamiento y evaluacion de modelos
  doParallel,     # Paralelizacion
  MLeval,         # Funciones para evaluar modelos con métricas gráficos.
  #MLmetrics,     # Colección de métricas de evaluación para modelos de machine learning.
  pROC,
  rpart,
  rpart.plot,
  gbm
)

```

## Carga de los datos 


```{r Reading poverty files}

train_dataset <- readRDS("train_dataset.rds")
test_dataset <- readRDS("test_dataset.rds")

train_dataset <- train_dataset |> 
  mutate(Pobre = factor(Pobre, levels = c(1, 0), labels = c("Pobre", "No_pobre")))

train_dataset |> summary()
```

## Exploración de datos

```{r, echo=FALSE}

skim(train_dataset)
skim(test_dataset)

train_dataset$actividad_jefe[is.na(train_dataset$actividad_jefe)] <- 6

test_dataset$num_ocupados[is.na(test_dataset$num_ocupados)] <- "mas_de_8"

test_dataset$actividad_jefe[is.na(test_dataset$actividad_jefe)] <- 6

```

```{r}

ggplot(train_dataset, aes(x = Pobre, fill = Pobre)) +
  geom_bar() + 
  theme_minimal() +  
  scale_fill_manual(values = c("Pobre" = "orange", "No_pobre"= "blue")) +  
  labs(x = "", y = "# de Personas")  

```

```{r Creating metric function}

multiStats <- function(...) c(twoClassSummary(...), defaultSummary(...), prSummary(...), multiClassSummary(...))

```


## 1. Logistic Regression

```{r}
# Identificacion de variables 

ctrl_rfe <- rfeControl(
  functions = lrFuncs,  
  method = "cv",             
  number = 10,                
  verbose = FALSE
)

ctrl_rfe$functions$summary <- multiStats

set.seed(1231)

rfe_logit <- rfe(x = model.matrix(Pobre ~ . - 1, data = train_dataset[, -c(1,3,8,9,10,11,13)]),
                 y = train_dataset$Pobre,
                 metric = "F1",
                 rfeControl = ctrl_rfe,
                 sizes = seq(5, 30, by = 5), 
                 maximize = TRUE)


# Control para train
ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = multiStats,
                     classProbs = TRUE,
                     verbose = FALSE,
                     savePredictions = TRUE)

# Entrenar con glm binomial
set.seed(1231)

x_variables <- c(colnames(train_dataset)[-c(1,3,8,9,10,11,13,14)])

glm_caret <- train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
                   data = train_dataset,
                   method = "glm",
                   trControl = ctrl,
                   family = "binomial")

```


```{r}
set.seed(1231)

model_2 <- c("median_education", "costo_vivienda", "recibe_subsidios",
             "num_cuartos","num_ocupados", "vulnerabilidad", "regimen_subsidiado",
             "hacinamiento", "num_dependientes", "Nper", "recibe_remesas", "tipo_posesion","int_subempleo")


logit_2 <- train(formula(paste0("Pobre ~", paste0(model_2, collapse = " + "))),
                   data = train_dataset,
                   method = "glm",
                   trControl = ctrl,
                   family = "binomial")

prediction = predict(logit_2, newdata = test_dataset)

results_2 = data.frame("id"= test_dataset$id, 
                       "pobre"= ifelse(predict(logit_2, newdata = test_dataset, type = "raw") =="Pobre", 1,0))

write.csv(results_2,"LGIT.csv", row.names = FALSE)

```


```{r}

set.seed(1231)

model_3 <- c("median_education", "costo_vivienda", "recibe_subsidios",
             "num_cuartos","num_ocupados", "vulnerabilidad", "regimen_subsidiado",
             "hacinamiento", "num_dependientes", "Nper", "recibe_remesas", "tipo_posesion","int_subempleo", "Clase", "ing_nomonet", "tasa_dependencia")

train_dataset2 <- train_dataset |> 
  filter(Dominio != "BOGOTA")

logit_3 <- train(formula(paste0("Pobre ~", paste0(model_3, collapse = " + "))),
                   data = train_dataset2,
                   method = "glm",
                   trControl = ctrl,
                   family = "binomial")
```


## 3. Elastic Net 

```{r}

# EN sin desbalance de clases (modelo 3)

grid <- expand.grid(
  alpha = seq(0, 1, by = 0.25),  
  lambda = 10^seq(-6, -2, length = 10)  
)

grid_refinada <- expand.grid(
  alpha = seq(0.25, 0.55, by = 0.05), 
  lambda = 10^seq(-6, -4, length = 20)
)

set.seed(1231) 

EN_logit_1 <- train(
    formula(paste0("Pobre ~", paste0(model_3, collapse = " + "))), 
    method = "glmnet",  
    data = train_dataset2,  
    family = "binomial",  
    tuneGrid = grid_refinada,  
    preProcess = c("center", "scale"),
    trControl = ctrl,
    metric = "F1")

# alpha = 0.45; lambda = 1e-04

set.seed(1231) 

EN_logit_1 <- train(
    formula(paste0("Pobre ~", paste0(model_3, collapse = " + "))), 
    method = "glmnet",  
    data = train_dataset2,  
    family = "binomial",  
    tuneGrid = expand.grid(alpha = 0.45, lambda = 1e-04),  
    preProcess = c("center", "scale"),
    trControl = ctrl,
    metric = "F1")

EN_logit_1

prediction = predict(EN_logit_1, newdata = test_dataset)

results_2 = data.frame("id"= test_dataset$id, 
                       "pobre"= ifelse(predict(EN_logit_1, newdata = test_dataset, type = "raw") =="Pobre", 1,0))

write.csv(results_2,"EN_.csv", row.names = FALSE)


```



```{r}

# Alternative cutoff (modelo 2)

roc_obj_en<-roc(response = EN_logit_1$pred$obs,  # Valores reales de la variable objetivo
                predictor= EN_logit_1$pred$Pobre, # Probabilidades predichas por el modelo
                levels = c("No_pobre", "Pobre"),  # 
                direction = "<")  # "<" "Pobre" es positivo

rfThresh_en <- coords(roc_obj_en, x = "best", best.method = "closest.topleft")
rfThresh_en

prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))

prec_recall<- prec_recall  |>  mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

results_2 = data.frame("id"= test_dataset$id, 
                       "pobre"= factor(
                         ifelse(
                           predict(
                             EN_logit_1, newdata = test_dataset,
                             type = "prob")$Pobre >=
                             prec_recall$threshold[which.max(prec_recall$F1)],1,0),
                                                                 levels = c(1,0), labels = c("Pobre","No_pobre"))) |> 
  mutate(pobre = case_when(
    pobre == "No_pobre" ~ 0,
    pobre == "Pobre" ~ 1
  ))

write.csv(results_2,"EN_lambda_1_e04_alpha_045.csv", row.names = FALSE)

```


```{r}

# SMOTE 

ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = multiStats,
                     classProbs = TRUE,
                     verbose = FALSE,
                     savePredictions = TRUE, 
                     sampling = "smote")

Logit_2_smote <- train(
     formula(paste0("Pobre ~", paste0(model_3, collapse = " + "))), 
     method = "glmnet",  
     data = train_dataset2,  
     family = "binomial",  
     trControl = ctrl,
     metric = "F1", 
       )

roc_obj_en<-roc(response = Logit_2_smote$pred$obs[Logit_2_smote$pred$lambda == Logit_2_smote$bestTune$lambda],  # Valores reales de la variable objetivo
                predictor= Logit_2_smote$pred$Pobre[Logit_2_smote$pred$lambda == Logit_2_smote$bestTune$lambda], # Probabilidades predichas por el modelo
                levels = c("No_pobre", "Pobre"),  #  
                direction = "<")  # "<" "Pobre" es positivo

rfThresh_en <- coords(roc_obj_en, x = "best", best.method = "closest.topleft")
rfThresh_en

prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))

prec_recall<- prec_recall  |>  mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

results_2 = data.frame("id"= test_dataset$id, 
                       "pobre"= factor(
                         ifelse(
                           predict(
                             Logit_2_smote, newdata = test_dataset,
                             type = "prob")$Pobre >=
                             prec_recall$threshold[which.max(prec_recall$F1)],1,0),
                                                                 levels = c(1,0), labels = c("Pobre","No_pobre"))) |> 
  mutate(pobre = case_when(
    pobre == "No_pobre" ~ 0,
    pobre == "Pobre" ~ 1
  ))

write.csv(results_2,"EN_lambda_1_e04_alpha_045.csv", row.names = FALSE)
```

## 3. CARTs

```{r}
my_tree <- rpart(formula(paste0("Pobre ~", paste0(model_3, collapse = " + "))),data= train_dataset2, control = list(maxdepth = 50))
prp(my_tree)

fitControl<-trainControl(method ="cv",
                         number=5, 
                         summaryFunction = multiStats,
                         classProbs = TRUE)

set.seed(1231)

tree_rpart2 <- train(
    formula(paste0("Pobre ~", paste0(model_3, collapse = "+"))),
    data=train_dataset2,
    method = "rpart2",
    trControl = fitControl,
    metric = "F1",
    tuneGrid = expand.grid(maxdepth = seq(1,15,1)))

set.seed(1231)

tree_rpart3 <- train(
    formula(paste0("Pobre ~", paste0(model_3, collapse = "+"))),
    data=train_dataset2,
    method = "rpart",
    trControl = fitControl,
    metric = "F1",
    tuneLength = 20)

```


## 4. Random Forest

## 5. ADA Boost

## 6. Grad Boost

```{r}
grid_gbm <- expand.grid(
  n.trees = c(300, 500, 1000),
  interaction.depth = c(2, 4, 6),
  shrinkage = c(0.001, 0.005, 0.01),
  n.minobsinnode = c(10, 20)
)


grid_gbm


fitControl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = multiStats,  # La función que ya armaste antes
  savePredictions = "final"
)


set.seed(1231)
gbm_tree <- train(formula(paste0("Pobre ~", paste0(model_3, collapse = "+"))),
                  data=train_dataset2,
                  method = "gbm", 
                  trControl = fitControl,
                  tuneGrid=grid_gbm,
                  verbose = FALSE)




```


## 7. XGBoost

## 8. Baile Bayes

## 9. LDA/QDA

## 10. Naive Bayes

## 11. KNN

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(Logit_2_smote)
```

