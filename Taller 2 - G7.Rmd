---
title: "Taller 2 - G7"
author: "Grupo 7"
date: "2025-04-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(pacman)
p_load(
  readxl,         # Read excel files
  tidyverse,      # Tidy data  
  skimr,          # Data summary  
  ranger,
  caret,         # Entrenamiento y evaluacion de modelos
  doParallel,     # Paralelizacion
  MLeval,         # Funciones para evaluar modelos con métricas gráficos.
  MLmetrics,     # Colección de métricas de evaluación para modelos de machine learning.
  pROC,
  adabag,
  e1071
)

```

## Carga de los datos 


```{r Reading poverty files}

train_dataset <- read_xlsx("train_dataset.xlsx")
test_dataset <- read_xlsx("test_dataset.xlsx")

# Pobre as a factor 
train_dataset <- train_dataset %>%
  mutate(Pobre = factor(Pobre, levels = c(1, 0), labels = c("Pobre", "No_pobre")))
```

## Exploración de datos

```{r, echo=FALSE}

skim(train_dataset)
skim(test_dataset)

train_dataset <- train_dataset |> 
  mutate(actividad_jefe = ifelse(is.na(actividad_jefe),"6", actividad_jefe))

```

```{r}

ggplot(train_dataset, aes(x = Pobre, fill = Pobre)) +
  geom_bar() + 
  theme_minimal() +  
  scale_fill_manual(values = c("Pobre" = "orange", "No_pobre"= "blue")) +  
  labs(x = "", y = "# de Personas")  

```

```{r Creating metric function}

multiStats <- function(...) c(twoClassSummary(...), defaultSummary(...), prSummary(...), multiClassSummary(...))

```


## 1. Logistic Regression

```{r}
# Identificacion de variables 

ctrl_rfe <- rfeControl(
  functions = lrFuncs,  
  method = "cv",             
  number = 10,                
  verbose = FALSE
)

set.seed(1231)


rfe_logit <- rfe(x = train_dataset[,-c(1,3,8,9,10,11,13,14)],
                 y = train_dataset$Pobre,
                 metric = "F1",
                 rfeControl = ctrl_rfe,
                 sizes = seq(5, 30, by = 5), 
                 maximize = TRUE)


# Control para train
ctrl <- trainControl(method = "cv",
                     number = 10,
                     summaryFunction = multiStats,
                     classProbs = TRUE,
                     verbose = FALSE,
                     savePredictions = TRUE)

# Entrenar con glm binomial
set.seed(1231)

x_variables <- c(colnames(train_dataset)[-c(1,3,8,9,10,11,13,14)])

glm_caret <- train(formula(paste0("Pobre ~", paste0(x_variables, collapse = " + "))),
                   data = train_dataset,
                   method = "glm",
                   trControl = ctrl,
                   family = "binomial")

```
```{r}
set.seed(1231)

model_2 <- c("median_education", "costo_vivienda", "recibe_subsidios",
             "num_cuartos","num_ocupados", "vulnerabilidad", "regimen_subsidiado",
             "hacinamiento", "num_dependientes", "Nper", "recibe_remesas", "tipo_posesion","int_subempleo")

logit_2 <- train(formula(paste0("Pobre ~", paste0(model_2, collapse = " + "))),
                   data = train_dataset,
                   method = "glm",
                   trControl = ctrl,
                   family = "binomial")

results_2 = data.frame("id"= test_dataset$id, 
                       "pobre"= ifelse(predict(logit_2, newdata = test_dataset, type = "raw") =="Pobre", 1,0))

 

write.csv(results_2,"LGIT.csv", row.names = FALSE)


set.seed(1231)

model_3 <- c("sexo_jefe","poly(as.numeric(edad_jefe),2,raw=TRUE)", "median_education", "costo_vivienda", "recibe_subsidios",
             "num_cuartos","num_ocupados", "vulnerabilidad", "regimen_subsidiado",
             "hacinamiento", "num_dependientes", "Nper", "recibe_remesas", "tipo_posesion","int_subempleo")

logit_3 <- train(formula(paste0("Pobre ~", paste0(model_3, collapse = " + "))),
                   data = train_dataset,
                   method = "glm",
                   trControl = ctrl,
                   family = "binomial")

```


## 3. Elastic Net

## 3. CARTs

## 4. Random Forest
```{r, echo=FALSE}

# Select only the specified predictors and the target variable for training
predictors <- c("tasa_dependencia", "nper", "median_education", "hacinamiento", 
                "costo_vivienda", "vulnerabilidad", "edad_jefe")

# Train the model using ranger
set.seed(42)
rf_model_1 <- ranger(
  Pobre ~ tasa_dependencia + Nper + median_education + hacinamiento + 
          costo_vivienda + vulnerabilidad + edad_jefe,
  data = train_dataset,
  num.trees = 500,
  mtry = floor(sqrt(length(predictors))),
  importance = 'impurity',
  probability = FALSE,
  classification = TRUE
)

# Variable importance
var_importance <- importance(rf_model_1)
var_importance_df <- data.frame(
  Variable = names(var_importance),
  Importance = var_importance
)

print(var_importance_df[order(-var_importance_df$Importance),])
print(rf_model_1)
#We got a OOB prediction error of 14,18%, lets check if we can improve this. 


# Basic hyperparameter tuning for mtry
mtry_values <- c(2, 3, 4, 5)
best_oob_error <- Inf
best_mtry <- NULL

for(m in mtry_values) {
  temp_model <- ranger(
    Pobre ~ tasa_dependencia + Nper + median_education + hacinamiento + 
            costo_vivienda + vulnerabilidad + edad_jefe,
    data = train_dataset,
    num.trees = 500,
    mtry = m,
    importance = 'impurity',
    classification = TRUE
  )
  
  oob_error <- temp_model$prediction.error
  cat("mtry =", m, "OOB error rate =", oob_error, "\n")
  
  if(oob_error < best_oob_error) {
    best_oob_error <- oob_error
    best_mtry <- m
  }
}

cat("Best mtry value:", best_mtry, "with OOB error:", best_oob_error, "\n")
#Best mtry value: 2 with OOB error: 0.1417374 

final_model_1 <- ranger(
  Pobre ~ tasa_dependencia + Nper + median_education + hacinamiento + 
          costo_vivienda + vulnerabilidad + edad_jefe,
  data = train_dataset,
  num.trees = 500,
  mtry = best_mtry,
  importance = 'impurity',
  classification = TRUE
)

print(final_model_1)

# Make predictions on the test set
test_predictions <- predict(final_model_1, data = test_dataset)

# Calculate F1 score from OOB predictions (to estimate Kaggle performance)
train_preds_oob <- predict(final_model_1, data = train_dataset)$predictions
conf_matrix <- table(Predicted = train_preds_oob, Actual = train_dataset$Pobre)
print(conf_matrix)

# Calculate metrics using OOB predictions
true_pos <- conf_matrix["1", "1"]
false_pos <- conf_matrix["1", "0"]
false_neg <- conf_matrix["0", "1"]

precision <- true_pos / (true_pos + false_pos)
recall <- true_pos / (true_pos + false_neg)
f1 <- 2 * precision * recall / (precision + recall)

cat("OOB Performance Metrics:\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1, "\n")

# Create submission dataframe
submission <- data.frame(
  id = test_dataset$id,
  Pobre = as.integer(levels(test_predictions$predictions)[test_predictions$predictions])
)

# Save the results in the required format
write.csv(submission, "RF_mtry_2.csv", row.names = FALSE, quote = FALSE)

# Verify the format
head(submission)
```

## 5. ADA Boost
```{r, echo=FALSE}
predictors <- c("tasa_dependencia", "Nper", "median_education", "hacinamiento",
                "costo_vivienda", "vulnerabilidad", "edad_jefe")


set.seed(123)  # For reproducibility
adaboost_model <- boosting(
  formula = as.formula(paste("Pobre", "~", paste(predictors, collapse = "+"))),
  data = train_dataset,
  mfinal = 50,  # Number of iterations
  control = rpart.control(maxdepth = 3)  # Limit tree depth to avoid overfitting
)

print(adaboost_model)
```
## 6. Grad Boost

## 7. XGBoost

## 8. Baile Bayes
```{r, echo=FALSE}
set.seed(456)
k <- 5  

# Creating folds
folds <- cut(seq(1, nrow(train_dataset)), breaks = k, labels = FALSE)

#Metrics
f1_scores <- numeric(k)

#Predictors
predictors <- c("tasa_dependencia", "Nper", "median_education", "hacinamiento", 
                "costo_vivienda", "vulnerabilidad", "edad_jefe","recibe_subsidios",
                "num_dependientes", "recibe_remesas", "num_dormitorios","informal")

# Loop de cross-validation
for(i in 1:k){
  test_indexes <- which(folds == i, arr.ind = TRUE)
  fold_test <- train_dataset[test_indexes, ]
  fold_train <- train_dataset[-test_indexes, ]
  
  nb_model <- naiveBayes(fold_train[, predictors], fold_train$Pobre)
  
  preds <- predict(nb_model, fold_test[, predictors])
  
  cm <- confusionMatrix(preds, fold_test$Pobre, positive = "Pobre")
  prec <- cm$byClass["Pos Pred Value"]
  rec  <- cm$byClass["Sensitivity"]
  f1 <- 2 * prec * rec / (prec + rec)
  f1_scores[i] <- f1
}

# F1
cat("F1 Score", k, "folds:", round(mean(f1_scores), 4), "\n")

# Predictions
test_preds_2 <- predict(nb_model, test_dataset[, predictors])
test_preds_numeric <- ifelse(test_preds_2 == "Pobre", 1, 0)

# Submission
submission <- data.frame(
  id = test_dataset$id,
  Pobre = test_preds_numeric
)
write.csv(submission, "NB_laplace_0_kernel_FALSE_vars_12.csv", row.names = FALSE)

# Verify the format
head(submission)
```
## 9. LDA/QDA

## 10. Naive Bayes

## 11. KNN

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

